# 部署

安装docker
```
wget http://mirrors.bclinux.org/bigcloud/2017.02/onest/docker/docker.tar.gz
tar xzvf docker.tar.gz -C /tmp
yum localinstall /tmp/docker/*.rpm
systemctl start docker && systemctl enable docker
wget -O /tmp/centos7-ansible-sshpaas.tar http://mirrors.bclinux.org/bigcloud/2017.02/onest/docker/centos7-ansible-sshpaas.tar
docker load -i /tmp/centos7-ansible-sshpaas.tar 
git clone http://10.130.15.14/oNest/ceph-ansible  /root
wget -O /root/ceph-ansible/tar/docker.tar.gz http://mirrors.bclinux.org/bigcloud/2017.02/onest/docker/docker.tar.gz
wget -O /root/ceph-ansible/tar/rgw.tar http://mirrors.bclinux.org/bigcloud/2017.02/onest/docker/rgw.tar 
wget -O /root/ceph-ansible/tar/yum-mon.tar.gz http://mirrors.bclinux.org/bigcloud/2017.02/onest/offline/yum-mon.tar.gz 
wget -O /root/ceph-ansible/tar/yum-osd.tar.gz http://mirrors.bclinux.org/bigcloud/2017.02/onest/offline/yum-osd.tar.gz 
wget -O /root/ceph-ansible/tar/yum-rgw.tar.gz http://mirrors.bclinux.org/bigcloud/2017.02/onest/offline/yum-rgw.tar.gz 
wget -O /root/ceph-ansible/tar/ceph-rest-api-1.4.2-1.noarch.rpm http://mirrors.bclinux.org/bigcloud/2017.01/onest/x86_64/Packages/ceph/ceph-rest-api-1.4.2-1.noarch.rpm 
docker run -dit --name ansible -v /root/ceph-ansible:/root/ceph-ansible  kirago/centos7-ansible-sshpaas:2.2.1
docker exec -it ansible bash
$开头表示在docker内执行
$cd /root/ceph-ansible
```

```
#编辑playbook
$vi site.yml
---
- hosts: mons
  tasks:
  - name: Ensure /tmp/ceph exist
    file:
        path: "/tmp/ceph"
        state: directory

  - name: copy ceph-mon rpms
    local_action: "shell sshpass -p {{ ansible_password }} scp -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null {{ playbook_dir }}/tar/yum-mon.tar.gz {{ ansible_user }}@{{ ansible_host }}:/tmp/yum-mon.tar.gz"

  - name: Unarchive yum-mon.tar.gz
    shell: "tar -xzvf /tmp/yum-mon.tar.gz -C /tmp/ceph"

  - name: install rpms
    command: "sh -c 'yum localinstall -y /tmp/ceph/yum-mon/*.rpm'"

- hosts: osds
  tasks:
  - name: Ensure /tmp/ceph exist
    file:
        path: "/tmp/ceph"
        state: directory

  - name: copy ceph-osd rpms
    local_action: "shell sshpass -p {{ ansible_password }} scp -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null {{ playbook_dir }}/tar/yum-osd.tar.gz {{ ansible_user }}@{{ ansible_host }}:/tmp/yum-osd.tar.gz"

  - name: Unarchive yum-osd.tar.gz
    shell: "tar -xzvf /tmp/yum-osd.tar.gz -C /tmp/ceph"

  - name: install rpms
    command: "sh -c 'yum localinstall -y /tmp/ceph/yum-osd/*.rpm'"

- hosts: rgws
  tasks:
  - name: Ensure /tmp/docker exist
    file:
        path: "/tmp/docker"
        state: directory

  - name: copy docker rpms
    local_action: "shell sshpass -p {{ ansible_password }} scp -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null {{ playbook_dir }}/tar/docker.tar.gz {{ ansible_user }}@{{ ansible_host }}:/tmp/docker.tar.gz"
    when: rgw_containerized_deployment

  - name: Unarchive docker.tar.gz
    shell: "tar -xzvf /tmp/docker.tar.gz -C /tmp"
    when: rgw_containerized_deployment

  - name: install rpms
    command: "sh -c 'yum localinstall -y /tmp/docker/*.rpm'"

  - name: start docker
    service: name=docker.service state=started enabled=yes
    when: rgw_containerized_deployment

  - name: copy rgw.tar
    local_action: "shell sshpass -p {{ ansible_password }} scp -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null {{ playbook_dir }}/tar/rgw.tar {{ ansible_user }}@{{ ansible_host }}:/tmp/rgw.tar"
    when: rgw_containerized_deployment

  - name: load rgw.tar
    command: "docker load -i /tmp/rgw.tar"
    when: rgw_containerized_deployment

- hosts:
  - rgws
  tasks:
  - name: Ensure /tmp/ceph exist
    file:
        path: "/tmp/ceph"
        state: directory

  - name: copy ceph-rgw rpms
    local_action: "shell sshpass -p {{ ansible_password }} scp -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null {{ playbook_dir }}/tar/yum-rgw.tar.gz {{ ansible_user }}@{{ ansible_host }}:/tmp/yum-rgw.tar.gz"
    when: not rgw_containerized_deployment

  - name: Unarchive yum-rgw.tar.gz
    shell: "tar -xzvf /tmp/yum-rgw.tar.gz -C /tmp/ceph"
    when: not rgw_containerized_deployment

  - name: install rpms
    command: "sh -c 'yum localinstall -y /tmp/ceph/yum-rgw/*.rpm'"
    when: not rgw_containerized_deployment

- hosts: mons
  become: True
  roles:
  - ceph-mon
  vars:
    - devices: []

- hosts: osds
  become: True
  roles:
    - { role: ceph-osd, devices: "['/dev/sdb']" }

- hosts: rgws
  become: True
  roles:
  - ceph-rgw
  vars:
    - devices: "[]"
    - instances: ["rgw1","rgw2","rgw3","rgw4"]

- hosts:
  - restapis
  tasks:
  - name: Ensure /tmp/ceph exist
    file:
        path: "/tmp/ceph"
        state: directory

  - name: copy ceph-rest-api-1.4.2-1.noarch.rpm 
    local_action: "shell sshpass -p {{ ansible_password }} scp -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null {{ playbook_dir }}/tar/ceph-rest-api-1.4.2-1.noarch.rpm {{ ansible_user }}@{{ ansible_host }}:/tmp/ceph-rest-api-1.4.2-1.noarch.rpm"

  - name: install rpms
    command: "sh -c 'yum localinstall -y /tmp/ceph-rest-api-1.4.2-1.noarch.rpm'"

- hosts: restapis
  gather_facts: false
  become: True
  roles:
  - ceph-restapi


#编辑分组
$vi inventory
#规划：mon节点 node1 node2 node3, osd节点 node1 node2 node3 node4 node5 rgw节点 node1 restapis节点 node1
node1 ansible_host=192.168.153.162  ansible_password=qwe123  ip=192.168.153.162
node2 ansible_host=192.168.153.163  ansible_password=qwe123  ip=192.168.153.163
node3 ansible_host=192.168.153.164  ansible_password=qwe123  ip=192.168.153.164
node4 ansible_host=192.168.153.165  ansible_password=qwe123  ip=192.168.153.165
node5 ansible_host=192.168.153.166  ansible_password=qwe123  ip=192.168.153.166
[mons]
node1
node2
node3
[osds]
node1
node2
node3
node4
node5
[rgws]
node1
[restapis]
node1

$ansible-playbook site.yml -v -i inventory  --extra-vars  \
'{"ceph_stable": true, \ 
"ceph_origin": "distro", \
"monitor_address_block": "192.168.10.0/24", \
"journal_collocation": true, \
"journal_size": 10240, \
"public_network": "192.168.10.0/24", \
"cluster_network": "192.168.10.0/24", \
"cephx": true, \
"ntp_service_enabled": "false", \
"osd_crush_tunables_optimal": true \
}' -u onest -f 200
```

# 更新mon的脚本update_cephconf_mon.sh
```
$ansible-playbook push_conf_mon.yml -v  -i  inventory  --extra-vars \
'{"monitor_address_block": "192.168.10.0/24", \
"journal_size": 10240, \
"public_network": "192.168.10.0/24", \
"cluster_network": "192.168.10.0/24", \
"osd_crush_tunables_optimal": true , \
"fsid":"5396edbb-af1f-4042-9998-2063e1c21b35" \
}' -u onest -f 200
```
其中 push_conf_mon.yml 如下：

```
$cat push_conf_mon.yml 
---
# push ceph.conf to nodes
# Need to load the facts from mons because ceph-common need them to generate the ceph.conf
- hosts:
    - mons
  become: True
  roles:
    - { role: ceph-push-conf }
  vars:
    - devices: []
```
执行更新
```
$chmod +x update_cephconf_mon.sh &&　./update_cephconf_mon.sh
```

# 更新osd的脚本update_cephconf_osd.sh
```
$ansible-playbook push_conf_osd.yml -v  -i  inventory  --extra-vars \
'{"monitor_address_block": "192.168.10.0/24", \
"journal_size": 10240, \
"public_network": "192.168.10.0/24", \
"cluster_network": "192.168.10.0/24", \
"osd_crush_tunables_optimal": true , \
"fsid":"5396edbb-af1f-4042-9998-2063e1c21b35" \
}' -u onest -f 200
```
其中 push_conf_osd.yml 如下：

```
$cat push_conf_osd.yml 
---
# push ceph.conf to nodes
# Need to load the facts from mons because ceph-common need them to generate the ceph.conf
- hosts:
    - osds
  become: True
  roles:
    - { role: ceph-push-conf }
  vars:
    - devices: []
```
执行更新
```
$chmod +x update_cephconf_osd.sh &&　./update_cephconf_osd.sh
```


# 更新osd的脚本update_cephconf_rgw.sh
```
$ansible-playbook push_conf_rgw.yml -v  -i  inventory  --extra-vars \
'{"monitor_address_block": "192.168.10.0/24", \
"journal_size": 10240, \
"public_network": "192.168.10.0/24", \
"cluster_network": "192.168.10.0/24", \
"osd_crush_tunables_optimal": true , \
"fsid":"5396edbb-af1f-4042-9998-2063e1c21b35" \
}' -u onest -f 200
```

其中 push_conf_osd.yml 如下：

```
$cat push_conf_rgw.yml 
---
# push ceph.conf to nodes
# Need to load the facts from mons because ceph-common need them to generate the ceph.conf
- hosts:
    - rgws
  become: True
  roles:
    - { role: ceph-push-conf }
  vars:
    - devices: []
```
执行更新
```
$chmod +x update_cephconf_rgw.sh &&　./update_cephconf_rgw.sh
```

# lo网卡绑定vip

rgws组机器所有的机器上的lo网卡绑定3个vip:192.168.10.244,192.168.10.220,192.168.10.221
```
site.yml
---
- hosts: rgws
  vars:
    cephvips: ["192.168.10.244","192.168.10.220","192.168.10.221"]
  roles:
    - lvs
	
inventory文件
[rgws]
192.168.10.100

部署
$ansible-playbook -i inventory site.yml -u root 
```


# 配置互为主备keepalived


```
例子：
192.168.153.168 
192.168.153.169
两台上安装keepalived+lvs,192.168.153.168为vi2的主,vi1的备。192.168.153.169为vi1的主,vi2的备
vi1的vip是192.168.153.222
vi2的vip是192.168.153.223
配置2个端口组，80和7480。每个端口的后端的realserver组可以在real_servers数组中配置
```

```
vi1(vi1的vip是192.168.153.222)       master           backup
                                192.168.153.169    192.168.153.168
vi2(vi2的vip是192.168.153.223)       master           backup
                                192.168.153.168    192.168.153.169
```
host_vars

```
#192.168.153.168
---
master: 'vi2'   #对于vrrp instance vi2来说 192.168.153.168机器是master 

#192.168.153.169
---
master: 'vi1'   #对于vrrp instance vi1来说 192.168.153.169机器是master 

```

```
site.yml
---
- hosts: test
  become: True
  vars:
    - vrrp_instances: [
    {name: 'vi1',vip: '192.168.153.222',virtual_router_id: '222'},
    {name: 'vi2',vip: '192.168.153.223',virtual_router_id: '223'}
    ]
    - vrrp_instances_devices: 'ens33'
    - virtual_server_groups: [
    {name: 'RGW_BUSINESS_80', vips: ['192.168.153.222','192.168.153.223'], port: '80',real_servers: ['192.168.153.156']},
    {name: 'RGW_BUSINESS_7480', vips: ['192.168.153.222','192.168.153.223'], port: '7480',real_servers: ['192.168.153.156']}
    ]
  roles:
  - serv-keepalived
 
inventory文件
[test]
192.168.153.168
192.168.153.169

#部署
$ansible-playbook -i inventory site.yml -u root 
```
