diff --git a/roles/ceph-common/templates/ceph.conf.j2 b/roles/ceph-common/templates/ceph.conf.j2
index e278a67..f0b742c 100644
--- a/roles/ceph-common/templates/ceph.conf.j2
+++ b/roles/ceph-common/templates/ceph.conf.j2
@@ -3,21 +3,21 @@
 
 [global]
 {% if not cephx %}
-auth cluster required = none
-auth service required = none
-auth client required = none
-auth supported = none
+auth_cluster_required = none
+auth_service_required = none
+auth_client_required = none
+auth_supported = none
 {% endif %}
 {% if not mon_containerized_deployment_with_kv and not mon_containerized_deployment %}
 fsid = {{ fsid }}
 {% endif %}
-max open files = {{ max_open_files }}
+max_open_files = {{ max_open_files }}
 {% if common_single_host_mode is defined and common_single_host_mode %}
-osd crush chooseleaf type = 0
+osd_crush_chooseleaf_type = 0
 {% endif %}
 {# NOTE (leseb): the blank lines in-between are needed otherwise we won't get any line break #}
 {% if groups[mon_group_name] is defined %}
-mon initial members = {% for host in groups[mon_group_name] %}
+mon_initial_members = {% for host in groups[mon_group_name] %}
       {% if hostvars[host]['ansible_fqdn'] is defined and mon_use_fqdn -%}
         {{ hostvars[host]['ansible_fqdn'] }}
       {%- elif hostvars[host]['ansible_hostname'] is defined -%}
@@ -29,9 +29,9 @@ mon initial members = {% for host in groups[mon_group_name] %}
 
 {% if not mon_containerized_deployment and not mon_containerized_deployment_with_kv %}
 {% if monitor_address_block %}
-mon host = {% for host in groups[mon_group_name] %}{{ hostvars[host]['ansible_all_ipv4_addresses'] | ipaddr(monitor_address_block) | first }}{% if not loop.last %},{% endif %}{% endfor %}
+mon_host = {% for host in groups[mon_group_name] %}{{ hostvars[host]['ansible_all_ipv4_addresses'] | ipaddr(monitor_address_block) | first }}{% if not loop.last %},{% endif %}{% endfor %}
 {% elif groups[mon_group_name] is defined %}
-mon host = {% for host in groups[mon_group_name] %}
+mon_host = {% for host in groups[mon_group_name] %}
              {% set address = hostvars[host]['monitor_address'] if hostvars[host]['monitor_address'] is defined else monitor_address %}
              {% set interface = hostvars[host]['monitor_interface'] if hostvars[host]['monitor_interface'] is defined else monitor_interface %}
              {% if interface != "interface" %}
@@ -50,7 +50,7 @@ mon host = {% for host in groups[mon_group_name] %}
 {% if mon_containerized_deployment %}
 fsid = {{ fsid }}
 {% if groups[mon_group_name] is defined %}
-mon host = {% for host in groups[mon_group_name] %}
+mon_host = {% for host in groups[mon_group_name] %}
              {% set interface = ["ansible_",ceph_mon_docker_interface]|join %}
              {% if mon_containerized_deployment -%}
                 {{ hostvars[host][interface]['ipv4']['address'] }}
@@ -65,28 +65,34 @@ mon host = {% for host in groups[mon_group_name] %}
 {% endif %}
 
 {% if public_network is defined %}
-public network = {{ public_network }}
+public_network = {{ public_network }}
 {% endif %}
 {% if cluster_network is defined %}
-cluster network = {{ cluster_network }}
+cluster_network = {{ cluster_network }}
 {% endif %}
 
-[client.libvirt]
-admin socket = {{ rbd_client_admin_socket_path }}/$cluster-$type.$id.$pid.$cctid.asok # must be writable by QEMU and allowed by SELinux or AppArmor
-log file = {{ rbd_client_log_file }} # must be writable by QEMU and allowed by SELinux or AppArmor
+
+{% include 'global.conf.j2' %}
+
+{% include 'mon.conf.j2' %}
 
 [osd]
-osd mkfs type = {{ osd_mkfs_type }}
-osd mkfs options xfs = {{ osd_mkfs_options_xfs }}
-osd mount options xfs = {{ osd_mount_options_xfs }}
-osd journal size = {{ journal_size }}
+osd_mkfs_type = {{ osd_mkfs_type }}
+osd_mkfs_options xfs = {{ osd_mkfs_options_xfs }}
+osd_mount_options xfs = {{ osd_mount_options_xfs }}
+osd_journal_size = {{ journal_size }}
 {% if filestore_xattr_use_omap != None %}
-filestore xattr use omap = {{ filestore_xattr_use_omap }}
+filestore_xattr_use_omap = {{ filestore_xattr_use_omap }}
 {% elif osd_mkfs_type == "ext4" %}
-filestore xattr use omap = true
+filestore_xattr_use_omap = true
 {# else, default is false #}
 {% endif %}
 
+{% include 'osd.conf.j2' %}
+
+{% include 'client.conf.j2' %}
+
+
 {% if groups[mds_group_name] is defined %}
 {% for host in groups[mds_group_name] %}
 {% if hostvars[host]['ansible_fqdn'] is defined and mds_use_fqdn %}
@@ -99,49 +105,47 @@ host = {{ hostvars[host]['ansible_hostname'] }}
 {% endfor %}
 {% endif %}
 
-{% if groups[rgw_group_name] is defined %}
-{% for host in groups[rgw_group_name] %}
-{% if hostvars[host]['ansible_hostname'] is defined %}
-[client.rgw.{{ hostvars[host]['ansible_hostname'] }}]
-{% if radosgw_dns_name is defined %}
-rgw dns name = {{ radosgw_dns_name }}
-{% endif %}
-host = {{ hostvars[host]['ansible_hostname'] }}
-keyring = /var/lib/ceph/radosgw/{{ cluster }}-rgw.{{ hostvars[host]['ansible_hostname'] }}/keyring
-rgw socket path = /tmp/radosgw-{{ hostvars[host]['ansible_hostname'] }}.sock
-log file = /var/log/ceph/{{ cluster }}-rgw-{{ hostvars[host]['ansible_hostname'] }}.log
-rgw data = /var/lib/ceph/radosgw/{{ cluster }}-rgw.{{ hostvars[host]['ansible_hostname'] }}
-rgw frontends = civetweb port={{ radosgw_civetweb_bind_ip }}:{{ radosgw_civetweb_port }} num_threads={{ radosgw_civetweb_num_threads }}
-{% if radosgw_keystone %}
-rgw keystone url = {{ radosgw_keystone_url }}
-rgw keystone api version = {{ radosgw_keystone_api_version }}
-{% if radosgw_keystone_auth_method == 'admin_token' %}
-rgw keystone admin token = {{ radosgw_keystone_admin_token }}
-{% elif radosgw_keystone_auth_method == 'auth_token' %}
-rgw keystone admin user = {{ radosgw_keystone_admin_user }}
-rgw keystone admin password = {{ radosgw_keystone_admin_password }}
-rgw keystone admin tenant = {{ radosgw_keystone_admin_tenant }}
-rgw keystone admin domain = {{ radosgw_keystone_admin_domain }}
-{% endif %}
-rgw keystone accepted roles = {{ radosgw_keystone_accepted_roles }}
-rgw keystone token cache size = {{ radosgw_keystone_token_cache_size }}
-rgw keystone revocation interval = {{ radosgw_keystone_revocation_internal }}
-rgw s3 auth use keystone = {{ radosgw_s3_auth_use_keystone }}
-{% if radosgw_keystone_ssl | bool %}
-nss db path = {{ radosgw_nss_db_path }}
-{% endif %}
-{% endif %}
-{% endif %}
-{% endfor %}
-{% endif %}
 
-{% if groups[restapi_group_name] is defined %}
-[client.restapi]
-{% if restapi_interface != "interface" %}
-{% include 'client_restapi_interface.j2' %}
-{% else %}
-{% include 'client_restapi_address.j2' %}
-{% endif %}
-keyring = /var/lib/ceph/restapi/ceph-restapi/keyring
-log file = /var/log/ceph/ceph-restapi.log
-{% endif %}
+{% if rgw1 is defined %}                                                                        
+[client.rgw.{{ rgw1['instance_name'] }}]                                                        
+host = {{ ansible_hostname }}                                                                   
+keyring = /var/lib/ceph/radosgw/{{ cluster }}-rgw.{{ rgw1['instance_name'] }}/keyring           
+log_file = /var/log/ceph/{{ cluster }}-rgw-{{ rgw1['instance_name'] }}.log                      
+rgw_data = /var/lib/ceph/radosgw/{{ cluster }}-rgw.{{ rgw1['instance_name'] }}                  
+rgw_frontends = "civetweb port={{ rgw1['radosgw_civetweb_port'] }}"                             
+{% if rgw1['rgw_enable_usage_log'] is defined %}rgw_enable_usage_log = {{ rgw1['rgw_enable_usage_log'] }}                                       
+{% endif %}                                                                                     
+{% endif %}                                                                                     
+                                                                                                
+{% if rgw2 is defined %}                                                                        
+[client.rgw.{{ rgw2['instance_name'] }}]                                                        
+host = {{ ansible_hostname }}                                                                   
+keyring = /var/lib/ceph/radosgw/{{ cluster }}-rgw.{{ rgw2['instance_name'] }}/keyring           
+log_file = /var/log/ceph/{{ cluster }}-rgw-{{ rgw2['instance_name'] }}.log                      
+rgw_data = /var/lib/ceph/radosgw/{{ cluster }}-rgw.{{ rgw2['instance_name'] }}                  
+rgw_frontends = "civetweb port={{ rgw2['radosgw_civetweb_port'] }}"                             
+{% if rgw2['rgw_enable_usage_log'] is defined %}rgw_enable_usage_log = {{ rgw2['rgw_enable_usage_log'] }}                                       
+{% endif %}                                                                                     
+{% endif %}                                                                                     
+                                                                                                
+{% if rgw3 is defined %}                                                                        
+[client.rgw.{{ rgw3['instance_name'] }}]                                                        
+host = {{ ansible_hostname }}                                                                   
+keyring = /var/lib/ceph/radosgw/{{ cluster }}-rgw.{{ rgw3['instance_name'] }}/keyring           
+log_file = /var/log/ceph/{{ cluster }}-rgw-{{ rgw3['instance_name'] }}.log                      
+rgw_data = /var/lib/ceph/radosgw/{{ cluster }}-rgw.{{ rgw3['instance_name'] }}                  
+rgw_frontends = "civetweb port={{ rgw3['radosgw_civetweb_port'] }}"                             
+{% if rgw3['rgw_enable_usage_log'] is defined %}rgw_enable_usage_log = {{ rgw3['rgw_enable_usage_log'] }}                                       
+{% endif %}                                                                                     
+{% endif %}                                                                                     
+                                                                                                
+{% if rgw4 is defined %}                                                                        
+[client.rgw.{{ rgw4['instance_name'] }}]                                                        
+host = {{ ansible_hostname }}                                                                   
+keyring = /var/lib/ceph/radosgw/{{ cluster }}-rgw.{{ rgw4['instance_name'] }}/keyring           
+log_file = /var/log/ceph/{{ cluster }}-rgw-{{ rgw4['instance_name'] }}.log                      
+rgw_data = /var/lib/ceph/radosgw/{{ cluster }}-rgw.{{ rgw4['instance_name'] }}                  
+rgw_frontends = "civetweb port={{ rgw4['radosgw_civetweb_port'] }}"                             
+{% if rgw4['rgw_enable_usage_log'] is defined %}rgw_enable_usage_log = {{ rgw4['rgw_enable_usage_log'] }}                                       
+{% endif %}                                                                                     
+{% endif %}                                                                                     
diff --git a/roles/ceph-common/templates/client.conf.j2 b/roles/ceph-common/templates/client.conf.j2
new file mode 100644
index 0000000..19e459e
--- /dev/null
+++ b/roles/ceph-common/templates/client.conf.j2
@@ -0,0 +1,23 @@
+[client]
+rgw_max_chunk_size= 4194304
+rgw_md_log_max_shards = 128
+rgw_data_log_num_shards = 256
+rgw_cache_lru_size = 10000
+rgw_enable_ops_log = False
+rgw_enable_usage_log = True
+rgw_gc_max_objs = 512
+rgw_gc_obj_min_wait = 7200
+rgw_gc_processor_max_time = 3600
+rgw_gc_processor_period = 3600
+rgw_num_rados_handles = 2
+rgw_op_thread_timeout = 600
+rgw_override_bucket_index_max_shards = 128
+rgw_thread_pool_size = 500
+rgw_usage_max_shards = 128
+rgw_usage_max_user_shards = 1
+rgw_user_quota_sync_interval = 1800
+rgw_user_quota_sync_wait_time = 1800
+#rgw_zone = guangzhou-2_gz-zone1
+#rgw_zonegroup = guangzhou-2
+#rgw_realm = tmp
+#rgw_dns_name = eos-guangzhou-2.cmecloud.cn 
diff --git a/roles/ceph-common/templates/global.conf.j2 b/roles/ceph-common/templates/global.conf.j2
new file mode 100644
index 0000000..9fd54c8
--- /dev/null
+++ b/roles/ceph-common/templates/global.conf.j2
@@ -0,0 +1,44 @@
+debug_asok = 0/0
+debug_auth = 0/0
+debug_buffer = 0/0
+debug_civetweb = 0/0
+debug_client = 0/0
+debug_context = 0/0
+debug_crush = 0/0
+debug_filer = 0/0
+debug_filestore = 0/0
+debug_finisher = 0/0
+debug_hadoop = 0/0
+debug_heartbeatmap = 0/0
+debug_journal = 0/0
+debug_journaler = 0/0
+debug_lockdep = 0/0
+debug_mds = 0/0
+debug_mds_balancer = 0/0
+debug_mds_locker = 0/0
+debug_mds_log = 0/0
+debug_mds_log_expire = 0/0
+debug_mds_migrator = 0/0
+debug_mon = 0/0
+debug_monc = 0/0
+debug_ms = 0/0
+debug_objclass = 0/0
+debug_objectcacher = 0/0
+debug_objecter = 0/0
+debug_optracker = 0/0
+debug_osd = 0/0
+debug_paxos = 0/0
+debug_perfcounter = 0/0
+debug_rados = 0/0
+debug_rbd = 0/0
+debug_rgw = 0/0
+debug_throttle = 0/0
+debug_timer = 0/0
+debug_tp = 0/0
+ms_type = simple
+osd_pool_default_min_size = 1
+osd_pool_default_pg_num = 64
+osd_pool_default_pgp_num = 64
+osd_pool_default_size = 3
+mon_min_osdmap_epochs = 300
+osd_pool_erasure_code_stripe_width = 65536
diff --git a/roles/ceph-common/templates/mon.conf.j2 b/roles/ceph-common/templates/mon.conf.j2
new file mode 100644
index 0000000..44a75fb
--- /dev/null
+++ b/roles/ceph-common/templates/mon.conf.j2
@@ -0,0 +1,7 @@
+[mon]
+mon_osd_full_ratio = 0.95
+mon_osd_nearfull_ratio = 0.85
+mon_osd_adjust_heartbeat_grace = False
+mon_osd_adjust_down_out_interval = False
+mon_osd_down_out_interval = 43200
+mon_clock_drift_allowed = 0.15
diff --git a/roles/ceph-common/templates/osd.conf.j2 b/roles/ceph-common/templates/osd.conf.j2
new file mode 100644
index 0000000..71a86ec
--- /dev/null
+++ b/roles/ceph-common/templates/osd.conf.j2
@@ -0,0 +1,39 @@
+filestore_fd_cache_size = 16384
+filestore_max_sync_interval = 5
+filestore_min_sync_interval = 0.01
+filestore_omap_header_cache_size = 16384
+filestore_op_threads = 4
+filestore_queue_max_bytes = 1073741824
+filestore_queue_max_ops = 5000
+journal_max_write_bytes = 1073741824
+journal_max_write_entries = 5000
+leveldb_write_buffer_size = 33554432
+leveldb_compression = False
+leveldb_cache_size = 536870912
+leveldb_block_size = 65536
+osd_client_message_cap = 100
+osd_client_message_size_cap = 1073741824
+#osd_crush_update_on_start = False
+osd_client_op_priority = 63
+osd_heartbeat_grace = 20
+osd_heartbeat_interval = 6
+osd_heartbeat_min_peers = 10
+osd_journal_size = 1024
+osd_mon_heartbeat_interval = 30
+osd_op_complaint_time = 5
+osd_op_threads = 3
+osd_op_thread_timeout = 60
+osd_scrub_begin_hour = 1
+osd_scrub_end_hour = 5
+osd_scrub_max_interval = 604800
+osd_scrub_min_interval = 86400
+osd_scrub_chunk_max = 25
+osd_scrub_chunk_min = 5
+osd_scrub_load_threshold = 0.5
+osd_deep_scrub_interval = 2419200
+osd_deep_scrub_stride = 524288
+osd_recovery_threads = 1
+osd_recovery_op_priority = 3
+osd_recovery_max_single_start = 1
+osd_recovery_max_chunk = 4194304
+osd_recovery_max_active = 3
diff --git a/roles/ceph-mon/tasks/start_monitor.yml b/roles/ceph-mon/tasks/start_monitor.yml
index 2ad2318..4cf93c2 100644
--- a/roles/ceph-mon/tasks/start_monitor.yml
+++ b/roles/ceph-mon/tasks/start_monitor.yml
@@ -60,3 +60,10 @@
   when:
     - use_systemd
     - ceph_release_num.{{ ceph_release }} > ceph_release_num.hammer
+
+- name: ceph osd crush tunables optimal
+  command: ceph osd crush tunables optimal
+  changed_when: false
+  failed_when:  false
+  when: osd_crush_tunables_optimal 
+
diff --git a/roles/ceph-push-conf/defaults/main.yml b/roles/ceph-push-conf/defaults/main.yml
new file mode 100644
index 0000000..a998c4a
--- /dev/null
+++ b/roles/ceph-push-conf/defaults/main.yml
@@ -0,0 +1,373 @@
+---
+# You can override vars by using host or group vars
+
+###########
+# GENERAL #
+###########
+
+fetch_directory: fetch/
+cluster: ceph # cluster name
+
+###########
+# INSTALL #
+###########
+
+mon_group_name: mons
+osd_group_name: osds
+rgw_group_name: rgws
+mds_group_name: mdss
+nfs_group_name: nfss
+restapi_group_name: restapis
+rbdmirror_group_name: rbdmirrors
+client_group_name: clients
+iscsi_group_name: iscsigws
+
+# If check_firewall is true, then ansible will try to determine if the
+# Ceph ports are blocked by a firewall. If the machine running ansible
+# cannot reach the Ceph ports for some other reason, you may need or
+# want to set this to False to skip those checks.
+check_firewall: False
+
+# This variable determines if ceph packages can be updated.  If False, the
+# package resources will use "state=present".  If True, they will use
+# "state=latest".
+upgrade_ceph_packages: False
+
+# /!\ EITHER ACTIVE ceph_stable OR ceph_stable_uca OR ceph_dev OR ceph_custom /!\
+
+debian_package_dependencies:
+  - python-pycurl
+  - hdparm
+  - ntp
+
+centos_package_dependencies:
+  - python-pycurl
+  - hdparm
+#  - epel-release
+  - ntp
+  - python-setuptools
+  - libselinux-python
+
+redhat_package_dependencies:
+  - python-pycurl
+  - hdparm
+  - ntp
+  - python-setuptools
+
+# Enable the ntp service by default to avoid clock skew on
+# ceph nodes
+ntp_service_enabled: true
+
+# The list of ceph packages needed for debian.
+# This variable should only be changed if packages are not available from a given
+# install source or architecture.
+debian_ceph_packages:
+  - ceph
+  - ceph-common    #|
+  - ceph-fs-common #|--> yes, they are already all dependencies from 'ceph'
+  - ceph-fuse      #|--> however while proceding to rolling upgrades and the 'ceph' package upgrade
+  - libcephfs1     #|--> they don't get update so we need to force them
+
+# Whether or not to install the ceph-test package.
+ceph_test: False
+
+## Configure package origin
+#
+ceph_origin: 'upstream' # or 'distro' or 'local'
+# 'distro' means that no separate repo file will be added
+# you will get whatever version of Ceph is included in your Linux distro.
+# 'local' means that the ceph binaries will be copied over from the local machine
+
+# LOCAL CEPH INSTALLATION (ceph_origin==local)
+#
+# Path to DESTDIR of the ceph install
+#ceph_installation_dir: "/path/to/ceph_installation/"
+# Whether or not to use installer script rundep_installer.sh
+# This script takes in rundep and installs the packages line by line onto the machine
+# If this is set to false then it is assumed that the machine ceph is being copied onto will already have
+# all runtime dependencies installed
+#use_installer: false
+# Root directory for ceph-ansible
+#ansible_dir: "/path/to/ceph-ansible"
+
+ceph_use_distro_backports: false # DEBIAN ONLY
+
+# STABLE
+########
+
+# COMMUNITY VERSION
+ceph_stable: false # use ceph stable branch
+ceph_stable_key: https://download.ceph.com/keys/release.asc
+ceph_stable_release: jewel # ceph stable release
+ceph_stable_repo: "http://download.ceph.com/debian-{{ ceph_stable_release }}"
+
+######################################
+# Releases name to number dictionary #
+######################################
+ceph_release_num:
+  dumpling: 0.67
+  emperor: 0.72
+  firefly: 0.80
+  giant: 0.87
+  hammer: 0.94
+  infernalis: 9
+  jewel: 10
+  kraken: 11
+
+# Use the option below to specify your applicable package tree, eg. when using non-LTS Ubuntu versions
+# # for a list of available Debian distributions, visit http://download.ceph.com/debian-{{ ceph_stable_release }}/dists/
+# for more info read: https://github.com/ceph/ceph-ansible/issues/305
+#ceph_stable_distro_source:
+
+# This option is needed for _both_ stable and dev version, so please always fill the right version
+# # for supported distros, see http://download.ceph.com/rpm-{{ ceph_stable_release }}/
+ceph_stable_redhat_distro: el7
+
+# ENTERPRISE VERSION RED HAT STORAGE (from 1.3)
+# This version is only supported on RHEL >= 7.1
+# As of RHEL 7.1, libceph.ko and rbd.ko are now included in Red Hat's kernel
+# packages natively. The RHEL 7.1 kernel packages are more stable and secure than
+# using these 3rd-party kmods with RHEL 7.0. Please update your systems to RHEL
+# 7.1 or later if you want to use the kernel RBD client.
+#
+# The CephFS kernel client is undergoing rapid development upstream, and we do
+# not recommend running the CephFS kernel module on RHEL 7's 3.10 kernel at this
+# time. Please use ELRepo's latest upstream 4.x kernels if you want to run CephFS
+# on RHEL 7.
+#
+ceph_rhcs: false
+# This will affect how/what repositories are enabled depending on the desired
+# version. The previous version was 1.3. The current version is 2.
+ceph_rhcs_version: 2
+ceph_rhcs_cdn_install: false # assumes all the nodes can connect to cdn.redhat.com
+ceph_rhcs_iso_install: false # usually used when nodes don't have access to cdn.redhat.com
+#ceph_rhcs_iso_path:
+ceph_rhcs_mount_path: /tmp/rh-storage-mount
+ceph_rhcs_repository_path: /tmp/rh-storage-repo # where to copy iso's content
+
+
+# UBUNTU CLOUD ARCHIVE
+# This allows the install of Ceph from the Ubuntu Cloud Archive.  The Ubuntu Cloud Archive
+# usually has newer Ceph releases than the normal distro repository.
+#
+ceph_stable_uca: false
+#ceph_stable_repo_uca: "http://ubuntu-cloud.archive.canonical.com/ubuntu"
+#ceph_stable_openstack_release_uca: liberty
+#ceph_stable_release_uca: "{{ansible_lsb.codename}}-updates/{{ceph_stable_openstack_release_uca}}"
+
+# DEV
+# ###
+
+ceph_dev: false # use ceph development branch
+ceph_dev_key: https://download.ceph.com/keys/autobuild.asc
+ceph_dev_branch: master # development branch you would like to use e.g: master, wip-hack
+
+# supported distros are centos6, centos7, fc17, fc18, fc19, fc20, fedora17, fedora18,
+# fedora19, fedora20, opensuse12, sles0. (see http://gitbuilder.ceph.com/).
+# For rhel, please pay attention to the versions: 'rhel6 3' or 'rhel 4', the fullname is _very_ important.
+ceph_dev_redhat_distro: centos7
+
+# CUSTOM
+# ###
+
+# Use a custom repository to install ceph.  For RPM, ceph_custom_repo should be
+# a URL to the .repo file to be installed on the targets.  For deb,
+# ceph_custom_repo should be the URL to the repo base.
+ceph_custom: false # use custom ceph repository
+ceph_custom_repo: https://server.domain.com/ceph-custom-repo
+
+
+######################
+# CEPH CONFIGURATION #
+######################
+
+## Ceph options
+#
+# Each cluster requires a unique, consistent filesystem ID. By
+# default, the playbook generates one for you and stores it in a file
+# in `fetch_directory`. If you want to customize how the fsid is
+# generated, you may find it useful to disable fsid generation to
+# avoid cluttering up your ansible repo. If you set `generate_fsid` to
+# false, you *must* generate `fsid` in another way.
+fsid: "{{ cluster_uuid.stdout }}"
+generate_fsid: true
+
+cephx: true
+max_open_files: 131072
+
+## Client options
+#
+rbd_cache: "true"
+rbd_cache_writethrough_until_flush: "true"
+rbd_concurrent_management_ops: 20
+
+rbd_client_directories: true # this will create rbd_client_log_path and rbd_client_admin_socket_path directories with proper permissions
+
+# Permissions for the rbd_client_log_path and
+# rbd_client_admin_socket_path. Depending on your use case for Ceph
+# you may want to change these values. The default, which is used if
+# any of the variables are unset or set to a false value (like `null`
+# or `false`) is to automatically determine what is appropriate for
+# the Ceph version with non-OpenStack workloads -- ceph:ceph and 0770
+# for infernalis releases, and root:root and 1777 for pre-infernalis
+# releases.
+#
+# For other use cases, including running Ceph with OpenStack, you'll
+# want to set these differently:
+#
+# For OpenStack on RHEL, you'll want:
+#   rbd_client_directory_owner: "qemu"
+#   rbd_client_directory_group: "libvirtd" (or "libvirt", depending on your version of libvirt)
+#   rbd_client_directory_mode: "0755"
+#
+# For OpenStack on Ubuntu or Debian, set:
+#    rbd_client_directory_owner: "libvirt-qemu"
+#    rbd_client_directory_group: "kvm"
+#    rbd_client_directory_mode: "0755"
+#
+# If you set rbd_client_directory_mode, you must use a string (e.g.,
+# 'rbd_client_directory_mode: "0755"', *not*
+# 'rbd_client_directory_mode: 0755', or Ansible will complain: mode
+# must be in octal or symbolic form
+rbd_client_directory_owner: null
+rbd_client_directory_group: null
+rbd_client_directory_mode: null
+
+rbd_client_log_path: /var/log/ceph
+rbd_client_log_file: "{{ rbd_client_log_path }}/qemu-guest-$pid.log" # must be writable by QEMU and allowed by SELinux or AppArmor
+rbd_client_admin_socket_path: /var/run/ceph # must be writable by QEMU and allowed by SELinux or AppArmor
+
+## Monitor options
+#
+# You must define either monitor_interface or monitor_address. Preference
+# will go to monitor_interface if both are defined.
+monitor_interface: interface
+monitor_address: 0.0.0.0
+mon_use_fqdn: false # if set to true, the MON name used will be the fqdn in the ceph.conf
+
+## OSD options
+#
+journal_size: 10240 # OSD journal size in MB
+public_network: 0.0.0.0/0
+cluster_network: "{{ public_network }}"
+osd_mkfs_type: xfs
+osd_mkfs_options_xfs: -f -i size=2048
+osd_mount_options_xfs: noatime,largeio,inode64,swalloc
+osd_objectstore: filestore
+
+# xattrs. by default, 'filestore xattr use omap' is set to 'true' if
+# 'osd_mkfs_type' is set to 'ext4'; otherwise it isn't set. This can
+# be set to 'true' or 'false' to explicitly override those
+# defaults. Leave it 'null' to use the default for your chosen mkfs
+# type.
+filestore_xattr_use_omap: null
+
+## MDS options
+#
+mds_use_fqdn: false # if set to true, the MDS name used will be the fqdn in the ceph.conf
+mds_allow_multimds: false
+mds_max_mds: 3
+
+## Rados Gateway options
+#
+#radosgw_dns_name: your.subdomain.tld # subdomains used by radosgw. See http://ceph.com/docs/master/radosgw/config/#enabling-subdomain-s3-calls
+radosgw_frontend: civetweb # supported options are 'apache' or 'civetweb', also edit roles/ceph-rgw/defaults/main.yml
+radosgw_civetweb_port: 8080 # on Infernalis we get: "set_ports_option: cannot bind to 80: 13 (Permission denied)"
+radosgw_civetweb_bind_ip: "{{ ansible_default_ipv4.address }}"
+radosgw_civetweb_num_threads: 50
+radosgw_keystone: false # activate OpenStack Keystone options full detail here: http://ceph.com/docs/master/radosgw/keystone/
+#radosgw_keystone_url: # url:admin_port ie: http://192.168.0.1:35357
+radosgw_keystone_admin_token: password
+radosgw_keystone_accepted_roles: Member, _member_, admin
+radosgw_keystone_token_cache_size: 10000
+radosgw_keystone_revocation_internal: 900
+radosgw_s3_auth_use_keystone: "true"
+radosgw_nss_db_path: /var/lib/ceph/radosgw/ceph-radosgw.{{ ansible_hostname }}/nss
+# Toggle 100-continue support for Apache and FastCGI
+# WARNING: Changing this value will cause an outage of Apache while it is reinstalled on RGW nodes
+http_100_continue: false
+# Rados Gateway options
+redhat_distro_ceph_extra: centos6.4 # supported distros are centos6.3, centos6.4, centos6, fedora18, fedora19, opensuse12.2, rhel6.3, rhel6.4, rhel6.5, rhel6, sles11sp2
+email_address: foo@bar.com
+
+## REST API options
+#
+restapi_interface: "{{ monitor_interface }}"
+restapi_address: "{{ monitor_address }}"
+restapi_port: 5000
+
+## Testing mode
+# enable this mode _only_ when you have a single node
+# if you don't want it keep the option commented
+#common_single_host_mode: true
+
+
+###################
+# CONFIG OVERRIDE #
+###################
+
+# Ceph configuration file override.
+# This allows you to specify more configuration options
+# using an INI style format.
+# The following sections are supported: [global], [mon], [osd], [mds], [rgw]
+#
+# Example:
+# ceph_conf_overrides:
+#   global:
+#     foo: 1234
+#     bar: 5678
+#
+ceph_conf_overrides: {}
+
+
+#############
+# OS TUNING #
+#############
+
+disable_transparent_hugepage: true
+os_tuning_params:
+  - { name: kernel.pid_max, value: 4194303 }
+  - { name: fs.file-max, value: 26234859 }
+  - { name: vm.zone_reclaim_mode, value: 0 }
+  - { name: vm.vfs_cache_pressure, value: 50 }
+  - { name: vm.swappiness, value: 10 }
+  - { name: vm.min_free_kbytes, value: "{{ vm_min_free_kbytes }}" }
+
+
+##########
+# DOCKER #
+##########
+
+docker: false
+
+# Do not comment the following variables mon_containerized_deployment_* here. These variables are being used
+# by ceph.conf.j2 template. so it should always be defined
+mon_containerized_deployment_with_kv: false
+mon_containerized_deployment: false
+mon_containerized_default_ceph_conf_with_kv: false
+
+
+##################
+# Temporary Vars #
+##################
+# NOTE(SamYaple): These vars are set here to they are defined before use. They
+# should be removed after a refactor has properly seperated all the checks into
+# the appropriate roles.
+
+journal_collocation: False
+raw_multi_journal: False
+osd_directory: False
+bluestore: False
+dmcrypt_journal_collocation: False
+dmcrypt_dedicated_journal: False
+raw_journal_devices: {}
+
+osd_auto_discovery: False
+
+# Confiure the type of NFS gatway access.  At least one must be enabled for an
+# NFS role to be useful
+#
+# Set this to true to enable File access via NFS.  Requires an MDS role.
+nfs_file_gw: true
+# Set this to true to enable Object access via NFS. Requires an RGW role.
+nfs_obj_gw: false
diff --git a/roles/ceph-push-conf/handlers/main.yml b/roles/ceph-push-conf/handlers/main.yml
new file mode 100644
index 0000000..7313caa
--- /dev/null
+++ b/roles/ceph-push-conf/handlers/main.yml
@@ -0,0 +1 @@
+---
diff --git a/roles/ceph-push-conf/meta/main.yml b/roles/ceph-push-conf/meta/main.yml
new file mode 100644
index 0000000..41e7cea
--- /dev/null
+++ b/roles/ceph-push-conf/meta/main.yml
@@ -0,0 +1,21 @@
+galaxy_info:
+  author: Emilien Kenler <ekenler@wizcorp.jp>
+  description: This role allows setting up extra disks and their mount points
+  company: Wizcorp K.K.
+  license: MIT
+  min_ansible_version: 2.0.0
+  platforms:
+    - name: EL
+      versions:
+        - 6
+        - 7
+    - name: Debian
+      versions:
+        - wheezy
+        - jessie
+    - name: Ubuntu
+      versions:
+        - all
+  categories:
+    - system
+dependencies:
\ No newline at end of file
diff --git a/roles/ceph-push-conf/tasks/main.yml b/roles/ceph-push-conf/tasks/main.yml
new file mode 100644
index 0000000..e01fff4
--- /dev/null
+++ b/roles/ceph-push-conf/tasks/main.yml
@@ -0,0 +1,44 @@
+---
+- include: ssd_devices_osd_id.yml
+  when: ssd_devices_conf is defined
+
+- include: sata_devices_osd_id.yml
+  when: sata_devices_conf is defined
+
+- name: generate ceph configuration file
+  action: config_template
+  args:
+    src: ceph.conf.j2
+#    dest: /etc/ceph/ceph.conf.tmp
+    dest: /etc/ceph/ceph.conf
+    owner: "ceph"
+    group: "ceph"
+    mode: "0664"
+    config_overrides:
+    config_type: ini
+
+#---
+#- name: get osd path
+#  shell: "df | grep {{ item }} | awk '{print $6}'"
+#  with_items: ssd_devices
+#  changed_when: false
+#  failed_when: false
+#  register: ssd_osd_path
+#
+#- name: get osd id
+#  command: cat {{ item.stdout }}/whoami
+#  with_items: ssd_osd_path.results
+#  changed_when: false
+#  failed_when: false
+#  register: ssd_osd_id
+#
+##- name: debug
+##  debug: msg="{{ ssd_osd_id['results'] }}"
+#
+#- name: ssd id array genders
+#  set_fact:
+#    ssd_id_array: "{{ ssd_id_array | default([]) + [ item.stdout ] }}"
+#  with_items: ssd_osd_id.results
+
+#- name: debug
+#  debug: msg="{{ ssd_id_array }}"
diff --git a/roles/ceph-push-conf/tasks/sata_devices_osd_id.yml b/roles/ceph-push-conf/tasks/sata_devices_osd_id.yml
new file mode 100644
index 0000000..057a505
--- /dev/null
+++ b/roles/ceph-push-conf/tasks/sata_devices_osd_id.yml
@@ -0,0 +1,33 @@
+---
+- name: get osd path
+  shell: "df | grep {{ item }} | awk '{print $6}'"
+  with_items: sata_devices_conf
+  changed_when: false
+  failed_when: false
+  register: sata_osd_path
+  when:
+    - sata_devices_conf is defined
+
+- name: get osd id
+  command: cat {{ item.stdout }}/whoami
+  with_items: sata_osd_path.results
+  changed_when: false
+  failed_when: false
+  register: sata_osd_id
+  when:
+    - sata_osd_path is defined
+
+#- name: debug
+#  debug: msg="{{ ssd_osd_id['results'] }}"
+
+- name: sata id array genders
+  set_fact:
+    sata_id_array: "{{ sata_id_array | default([]) + [ item.stdout ] }}"
+  with_items: sata_osd_id.results
+  when:
+    - sata_osd_id is defined
+
+- name: debug
+  debug: msg="{{ sata_id_array }}"
+  when:
+    - sata_id_array is defined
diff --git a/roles/ceph-push-conf/tasks/ssd_devices_osd_id.yml b/roles/ceph-push-conf/tasks/ssd_devices_osd_id.yml
new file mode 100644
index 0000000..056b73d
--- /dev/null
+++ b/roles/ceph-push-conf/tasks/ssd_devices_osd_id.yml
@@ -0,0 +1,32 @@
+---
+- name: get osd path
+  shell: "df | grep {{ item }} | awk '{print $6}'"
+  with_items: ssd_devices_conf
+  changed_when: false
+  failed_when: false
+  register: ssd_osd_path
+  when:
+    - ssd_devices_conf is defined
+
+- debug: var=ssd_osd_path
+
+- name: get osd id
+  command: cat {{ item.stdout }}/whoami
+  with_items: ssd_osd_path.results
+  changed_when: false
+  failed_when: false
+  register: ssd_osd_id
+  when:
+    - ssd_osd_path is defined
+
+- name: ssd id array genders
+  set_fact:
+    ssd_id_array: "{{ ssd_id_array | default([]) + [ item.stdout ] }}"
+  with_items: ssd_osd_id.results
+  when:
+    - ssd_osd_id is defined
+
+- name: debug
+  debug: msg="{{ ssd_id_array }}"
+  when:
+    - ssd_id_array is defined
diff --git a/roles/ceph-push-conf/templates/ceph.conf.j2 b/roles/ceph-push-conf/templates/ceph.conf.j2
new file mode 100644
index 0000000..f0b742c
--- /dev/null
+++ b/roles/ceph-push-conf/templates/ceph.conf.j2
@@ -0,0 +1,151 @@
+#jinja2: trim_blocks: "true", lstrip_blocks: "true"
+# {{ ansible_managed }}
+
+[global]
+{% if not cephx %}
+auth_cluster_required = none
+auth_service_required = none
+auth_client_required = none
+auth_supported = none
+{% endif %}
+{% if not mon_containerized_deployment_with_kv and not mon_containerized_deployment %}
+fsid = {{ fsid }}
+{% endif %}
+max_open_files = {{ max_open_files }}
+{% if common_single_host_mode is defined and common_single_host_mode %}
+osd_crush_chooseleaf_type = 0
+{% endif %}
+{# NOTE (leseb): the blank lines in-between are needed otherwise we won't get any line break #}
+{% if groups[mon_group_name] is defined %}
+mon_initial_members = {% for host in groups[mon_group_name] %}
+      {% if hostvars[host]['ansible_fqdn'] is defined and mon_use_fqdn -%}
+        {{ hostvars[host]['ansible_fqdn'] }}
+      {%- elif hostvars[host]['ansible_hostname'] is defined -%}
+        {{ hostvars[host]['ansible_hostname'] }}
+      {%- endif %}
+      {%- if not loop.last %},{% endif %}
+    {% endfor %}
+{% endif %}
+
+{% if not mon_containerized_deployment and not mon_containerized_deployment_with_kv %}
+{% if monitor_address_block %}
+mon_host = {% for host in groups[mon_group_name] %}{{ hostvars[host]['ansible_all_ipv4_addresses'] | ipaddr(monitor_address_block) | first }}{% if not loop.last %},{% endif %}{% endfor %}
+{% elif groups[mon_group_name] is defined %}
+mon_host = {% for host in groups[mon_group_name] %}
+             {% set address = hostvars[host]['monitor_address'] if hostvars[host]['monitor_address'] is defined else monitor_address %}
+             {% set interface = hostvars[host]['monitor_interface'] if hostvars[host]['monitor_interface'] is defined else monitor_interface %}
+             {% if interface != "interface" %}
+               {% for key in hostvars[host].keys() %}
+                 {% if hostvars[host][key]['macaddress'] is defined and hostvars[host][key]['device'] is defined and hostvars[host][key]['device'] == interface -%}
+                    {{ hostvars[host][key]['ipv4']['address'] }}
+                 {%- endif %}
+               {% endfor %}
+             {% elif address != "0.0.0.0" -%}
+               {{ address }}
+             {%- endif %}
+             {%- if not loop.last %},{% endif %}
+           {% endfor %}
+{% endif %}
+{% endif %}
+{% if mon_containerized_deployment %}
+fsid = {{ fsid }}
+{% if groups[mon_group_name] is defined %}
+mon_host = {% for host in groups[mon_group_name] %}
+             {% set interface = ["ansible_",ceph_mon_docker_interface]|join %}
+             {% if mon_containerized_deployment -%}
+                {{ hostvars[host][interface]['ipv4']['address'] }}
+             {%- elif hostvars[host]['monitor_address'] is defined -%}
+                {{ hostvars[host]['monitor_address'] }}
+             {%- elif monitor_address != "0.0.0.0" -%}
+                {{ monitor_address }}
+             {%- endif %}
+             {%- if not loop.last %},{% endif %}
+           {% endfor %}
+{% endif %}
+{% endif %}
+
+{% if public_network is defined %}
+public_network = {{ public_network }}
+{% endif %}
+{% if cluster_network is defined %}
+cluster_network = {{ cluster_network }}
+{% endif %}
+
+
+{% include 'global.conf.j2' %}
+
+{% include 'mon.conf.j2' %}
+
+[osd]
+osd_mkfs_type = {{ osd_mkfs_type }}
+osd_mkfs_options xfs = {{ osd_mkfs_options_xfs }}
+osd_mount_options xfs = {{ osd_mount_options_xfs }}
+osd_journal_size = {{ journal_size }}
+{% if filestore_xattr_use_omap != None %}
+filestore_xattr_use_omap = {{ filestore_xattr_use_omap }}
+{% elif osd_mkfs_type == "ext4" %}
+filestore_xattr_use_omap = true
+{# else, default is false #}
+{% endif %}
+
+{% include 'osd.conf.j2' %}
+
+{% include 'client.conf.j2' %}
+
+
+{% if groups[mds_group_name] is defined %}
+{% for host in groups[mds_group_name] %}
+{% if hostvars[host]['ansible_fqdn'] is defined and mds_use_fqdn %}
+[mds.{{ hostvars[host]['ansible_fqdn'] }}]
+host = {{ hostvars[host]['ansible_fqdn'] }}
+{% elif hostvars[host]['ansible_hostname'] is defined %}
+[mds.{{ hostvars[host]['ansible_hostname'] }}]
+host = {{ hostvars[host]['ansible_hostname'] }}
+{% endif %}
+{% endfor %}
+{% endif %}
+
+
+{% if rgw1 is defined %}                                                                        
+[client.rgw.{{ rgw1['instance_name'] }}]                                                        
+host = {{ ansible_hostname }}                                                                   
+keyring = /var/lib/ceph/radosgw/{{ cluster }}-rgw.{{ rgw1['instance_name'] }}/keyring           
+log_file = /var/log/ceph/{{ cluster }}-rgw-{{ rgw1['instance_name'] }}.log                      
+rgw_data = /var/lib/ceph/radosgw/{{ cluster }}-rgw.{{ rgw1['instance_name'] }}                  
+rgw_frontends = "civetweb port={{ rgw1['radosgw_civetweb_port'] }}"                             
+{% if rgw1['rgw_enable_usage_log'] is defined %}rgw_enable_usage_log = {{ rgw1['rgw_enable_usage_log'] }}                                       
+{% endif %}                                                                                     
+{% endif %}                                                                                     
+                                                                                                
+{% if rgw2 is defined %}                                                                        
+[client.rgw.{{ rgw2['instance_name'] }}]                                                        
+host = {{ ansible_hostname }}                                                                   
+keyring = /var/lib/ceph/radosgw/{{ cluster }}-rgw.{{ rgw2['instance_name'] }}/keyring           
+log_file = /var/log/ceph/{{ cluster }}-rgw-{{ rgw2['instance_name'] }}.log                      
+rgw_data = /var/lib/ceph/radosgw/{{ cluster }}-rgw.{{ rgw2['instance_name'] }}                  
+rgw_frontends = "civetweb port={{ rgw2['radosgw_civetweb_port'] }}"                             
+{% if rgw2['rgw_enable_usage_log'] is defined %}rgw_enable_usage_log = {{ rgw2['rgw_enable_usage_log'] }}                                       
+{% endif %}                                                                                     
+{% endif %}                                                                                     
+                                                                                                
+{% if rgw3 is defined %}                                                                        
+[client.rgw.{{ rgw3['instance_name'] }}]                                                        
+host = {{ ansible_hostname }}                                                                   
+keyring = /var/lib/ceph/radosgw/{{ cluster }}-rgw.{{ rgw3['instance_name'] }}/keyring           
+log_file = /var/log/ceph/{{ cluster }}-rgw-{{ rgw3['instance_name'] }}.log                      
+rgw_data = /var/lib/ceph/radosgw/{{ cluster }}-rgw.{{ rgw3['instance_name'] }}                  
+rgw_frontends = "civetweb port={{ rgw3['radosgw_civetweb_port'] }}"                             
+{% if rgw3['rgw_enable_usage_log'] is defined %}rgw_enable_usage_log = {{ rgw3['rgw_enable_usage_log'] }}                                       
+{% endif %}                                                                                     
+{% endif %}                                                                                     
+                                                                                                
+{% if rgw4 is defined %}                                                                        
+[client.rgw.{{ rgw4['instance_name'] }}]                                                        
+host = {{ ansible_hostname }}                                                                   
+keyring = /var/lib/ceph/radosgw/{{ cluster }}-rgw.{{ rgw4['instance_name'] }}/keyring           
+log_file = /var/log/ceph/{{ cluster }}-rgw-{{ rgw4['instance_name'] }}.log                      
+rgw_data = /var/lib/ceph/radosgw/{{ cluster }}-rgw.{{ rgw4['instance_name'] }}                  
+rgw_frontends = "civetweb port={{ rgw4['radosgw_civetweb_port'] }}"                             
+{% if rgw4['rgw_enable_usage_log'] is defined %}rgw_enable_usage_log = {{ rgw4['rgw_enable_usage_log'] }}                                       
+{% endif %}                                                                                     
+{% endif %}                                                                                     
diff --git a/roles/ceph-push-conf/templates/client.conf.j2 b/roles/ceph-push-conf/templates/client.conf.j2
new file mode 100644
index 0000000..19e459e
--- /dev/null
+++ b/roles/ceph-push-conf/templates/client.conf.j2
@@ -0,0 +1,23 @@
+[client]
+rgw_max_chunk_size= 4194304
+rgw_md_log_max_shards = 128
+rgw_data_log_num_shards = 256
+rgw_cache_lru_size = 10000
+rgw_enable_ops_log = False
+rgw_enable_usage_log = True
+rgw_gc_max_objs = 512
+rgw_gc_obj_min_wait = 7200
+rgw_gc_processor_max_time = 3600
+rgw_gc_processor_period = 3600
+rgw_num_rados_handles = 2
+rgw_op_thread_timeout = 600
+rgw_override_bucket_index_max_shards = 128
+rgw_thread_pool_size = 500
+rgw_usage_max_shards = 128
+rgw_usage_max_user_shards = 1
+rgw_user_quota_sync_interval = 1800
+rgw_user_quota_sync_wait_time = 1800
+#rgw_zone = guangzhou-2_gz-zone1
+#rgw_zonegroup = guangzhou-2
+#rgw_realm = tmp
+#rgw_dns_name = eos-guangzhou-2.cmecloud.cn 
diff --git a/roles/ceph-push-conf/templates/client_restapi_address.j2 b/roles/ceph-push-conf/templates/client_restapi_address.j2
new file mode 100644
index 0000000..e7279a8
--- /dev/null
+++ b/roles/ceph-push-conf/templates/client_restapi_address.j2
@@ -0,0 +1,2 @@
+public addr = {{ hostvars[inventory_hostname]['restapi_address'] if hostvars[inventory_hostname]['restapi_address'] is defined else restapi_address }}:{{ restapi_port }}
+
diff --git a/roles/ceph-push-conf/templates/client_restapi_interface.j2 b/roles/ceph-push-conf/templates/client_restapi_interface.j2
new file mode 100644
index 0000000..6686de6
--- /dev/null
+++ b/roles/ceph-push-conf/templates/client_restapi_interface.j2
@@ -0,0 +1,2 @@
+public addr = {{ hostvars[inventory_hostname]['ansible_' + restapi_interface]['ipv4']['address'] }}:{{ restapi_port }}
+
diff --git a/roles/ceph-push-conf/templates/ganesha.conf.j2 b/roles/ceph-push-conf/templates/ganesha.conf.j2
new file mode 100644
index 0000000..0c661b3
--- /dev/null
+++ b/roles/ceph-push-conf/templates/ganesha.conf.j2
@@ -0,0 +1,58 @@
+#jinja2: trim_blocks: "true", lstrip_blocks: "true"
+# {{ ansible_managed }}
+
+{% if nfs_file_gw %}
+EXPORT
+{
+	Export_ID={{ ceph_nfs_ceph_export_id }};
+
+	Path = "/";
+
+	Pseudo = {{ ceph_nfs_ceph_pseudo_path }};
+
+	Access_Type = {{ ceph_nfs_ceph_access_type }};
+
+	NFS_Protocols = {{ ceph_nfs_ceph_protocols }};
+
+	Transport_Protocols = TCP;
+
+	Sectype = sys,krb5,krb5i,krb5p;
+
+	FSAL {
+		Name = CEPH;
+	}
+}
+{% endif %}
+{% if nfs_obj_gw %}
+EXPORT
+{
+	Export_ID={{ ceph_nfs_rgw_export_id }};
+
+	Path = "/";
+
+	Pseudo = {{ ceph_nfs_rgw_pseudo_path }};
+
+	Access_Type = {{ ceph_nfs_rgw_access_type }};
+
+	NFS_Protocols = {{ ceph_nfs_rgw_protocols }};
+
+	Transport_Protocols = TCP;
+
+	Sectype = sys,krb5,krb5i,krb5p;
+
+	FSAL {
+		Name = RGW;
+		User_Id = "{{ ceph_nfs_rgw_user }}";
+		Access_Key_Id ="{{ ceph_nfs_rgw_access_key }}";
+		Secret_Access_Key = "{{ ceph_nfs_rgw_secret_key }}";
+	}
+}
+{% endif %}
+
+LOG {
+        Facility {
+                name = FILE;
+                destination = "{{ ceph_nfs_log_file }}";
+                enable = active;
+        }
+}
diff --git a/roles/ceph-push-conf/templates/global.conf.j2 b/roles/ceph-push-conf/templates/global.conf.j2
new file mode 100644
index 0000000..9fd54c8
--- /dev/null
+++ b/roles/ceph-push-conf/templates/global.conf.j2
@@ -0,0 +1,44 @@
+debug_asok = 0/0
+debug_auth = 0/0
+debug_buffer = 0/0
+debug_civetweb = 0/0
+debug_client = 0/0
+debug_context = 0/0
+debug_crush = 0/0
+debug_filer = 0/0
+debug_filestore = 0/0
+debug_finisher = 0/0
+debug_hadoop = 0/0
+debug_heartbeatmap = 0/0
+debug_journal = 0/0
+debug_journaler = 0/0
+debug_lockdep = 0/0
+debug_mds = 0/0
+debug_mds_balancer = 0/0
+debug_mds_locker = 0/0
+debug_mds_log = 0/0
+debug_mds_log_expire = 0/0
+debug_mds_migrator = 0/0
+debug_mon = 0/0
+debug_monc = 0/0
+debug_ms = 0/0
+debug_objclass = 0/0
+debug_objectcacher = 0/0
+debug_objecter = 0/0
+debug_optracker = 0/0
+debug_osd = 0/0
+debug_paxos = 0/0
+debug_perfcounter = 0/0
+debug_rados = 0/0
+debug_rbd = 0/0
+debug_rgw = 0/0
+debug_throttle = 0/0
+debug_timer = 0/0
+debug_tp = 0/0
+ms_type = simple
+osd_pool_default_min_size = 1
+osd_pool_default_pg_num = 64
+osd_pool_default_pgp_num = 64
+osd_pool_default_size = 3
+mon_min_osdmap_epochs = 300
+osd_pool_erasure_code_stripe_width = 65536
diff --git a/roles/ceph-push-conf/templates/mon.conf.j2 b/roles/ceph-push-conf/templates/mon.conf.j2
new file mode 100644
index 0000000..44a75fb
--- /dev/null
+++ b/roles/ceph-push-conf/templates/mon.conf.j2
@@ -0,0 +1,7 @@
+[mon]
+mon_osd_full_ratio = 0.95
+mon_osd_nearfull_ratio = 0.85
+mon_osd_adjust_heartbeat_grace = False
+mon_osd_adjust_down_out_interval = False
+mon_osd_down_out_interval = 43200
+mon_clock_drift_allowed = 0.15
diff --git a/roles/ceph-push-conf/templates/osd.conf.j2 b/roles/ceph-push-conf/templates/osd.conf.j2
new file mode 100644
index 0000000..71a86ec
--- /dev/null
+++ b/roles/ceph-push-conf/templates/osd.conf.j2
@@ -0,0 +1,39 @@
+filestore_fd_cache_size = 16384
+filestore_max_sync_interval = 5
+filestore_min_sync_interval = 0.01
+filestore_omap_header_cache_size = 16384
+filestore_op_threads = 4
+filestore_queue_max_bytes = 1073741824
+filestore_queue_max_ops = 5000
+journal_max_write_bytes = 1073741824
+journal_max_write_entries = 5000
+leveldb_write_buffer_size = 33554432
+leveldb_compression = False
+leveldb_cache_size = 536870912
+leveldb_block_size = 65536
+osd_client_message_cap = 100
+osd_client_message_size_cap = 1073741824
+#osd_crush_update_on_start = False
+osd_client_op_priority = 63
+osd_heartbeat_grace = 20
+osd_heartbeat_interval = 6
+osd_heartbeat_min_peers = 10
+osd_journal_size = 1024
+osd_mon_heartbeat_interval = 30
+osd_op_complaint_time = 5
+osd_op_threads = 3
+osd_op_thread_timeout = 60
+osd_scrub_begin_hour = 1
+osd_scrub_end_hour = 5
+osd_scrub_max_interval = 604800
+osd_scrub_min_interval = 86400
+osd_scrub_chunk_max = 25
+osd_scrub_chunk_min = 5
+osd_scrub_load_threshold = 0.5
+osd_deep_scrub_interval = 2419200
+osd_deep_scrub_stride = 524288
+osd_recovery_threads = 1
+osd_recovery_op_priority = 3
+osd_recovery_max_single_start = 1
+osd_recovery_max_chunk = 4194304
+osd_recovery_max_active = 3
diff --git a/roles/ceph-push-conf/templates/redhat_storage_repo.j2 b/roles/ceph-push-conf/templates/redhat_storage_repo.j2
new file mode 100644
index 0000000..16f57c4
--- /dev/null
+++ b/roles/ceph-push-conf/templates/redhat_storage_repo.j2
@@ -0,0 +1,21 @@
+# {{ ansible_managed }}
+[rh_storage_mon]
+name=Red Hat Ceph Storage - local packages for Ceph monitor
+baseurl=file://{{ ceph_rhcs_repository_path }}/MON
+enabled=1
+gpgcheck=1
+priority=1
+
+[rh_storage_osd]
+name=Red Hat Ceph Storage - local packages for Ceph OSD
+baseurl=file://{{ ceph_rhcs_repository_path }}/OSD
+enabled=1
+gpgcheck=1
+priority=1
+
+[rh_storage_tools]
+name=Red Hat Ceph Storage - local packages for Ceph client, MDS, and RGW
+baseurl=file://{{ ceph_rhcs_repository_path }}/Tools
+enabled=1
+gpgcheck=1
+priority=1
diff --git a/roles/ceph-push-conf/templates/rhcs.pref.j2 b/roles/ceph-push-conf/templates/rhcs.pref.j2
new file mode 100644
index 0000000..45abfbc
--- /dev/null
+++ b/roles/ceph-push-conf/templates/rhcs.pref.j2
@@ -0,0 +1,7 @@
+#jinja2: trim_blocks: "true", lstrip_blocks: "true"
+# {{ ansible_managed }}
+
+Explanation: Prefer Red Hat packages
+Package: *
+Pin: release o=/Red Hat/
+Pin-Priority: 999
diff --git a/roles/ceph-rgw/tasks/pre_requisite.yml b/roles/ceph-rgw/tasks/pre_requisite.yml
index db4cee6..261bd3e 100644
--- a/roles/ceph-rgw/tasks/pre_requisite.yml
+++ b/roles/ceph-rgw/tasks/pre_requisite.yml
@@ -11,6 +11,18 @@
     - /var/lib/ceph/radosgw
     - /var/lib/ceph/radosgw/{{ cluster }}-rgw.{{ ansible_hostname }}
     - "{{ rbd_client_admin_socket_path }}"
+  when: instances is not defined
+
+- name: create rados gateway directories for rgw instances
+  file:
+    path: "/var/lib/ceph/radosgw/{{ cluster }}-rgw.{{ item | default(ansible_hostname) }}"
+    state: directory
+    owner: "{{ dir_owner }}"
+    group: "{{ dir_group }}"
+    mode: "{{ dir_mode }}"
+  with_items: "{{ instances }}"
+  when: instances is defined
+
 
 - name: copy rados gateway bootstrap key
   copy:
@@ -31,7 +43,21 @@
   args:
     creates: /var/lib/ceph/radosgw/{{ cluster }}-rgw.{{ ansible_hostname }}/keyring
   changed_when: false
-  when: cephx
+  when: 
+    - cephx
+    - instances is not defined
+
+- name: create rados gateway keyring for rgw instances
+  command: ceph --cluster {{ cluster }} --name client.bootstrap-rgw --keyring /var/lib/ceph/bootstrap-rgw/{{ cluster }}.keyring auth get-or-create client.rgw.{{ item | default(ansible_hostname) }} osd 'allow rwx' mon 'allow rw' -o /var/lib/ceph/radosgw/{{ cluster }}-rgw.{{ item | default(ansible_hostname) }}/keyring
+  args:
+    creates: /var/lib/ceph/radosgw/{{ cluster }}-rgw.{{ item | default(ansible_hostname) }}/keyring
+  changed_when: false
+  with_items: "{{ instances }}"
+  when:
+    - cephx
+    - instances is defined
+
+
 
 - name: set rados gateway key permissions (for or after the infernalis release)
   file:
@@ -39,7 +65,20 @@
     mode: "{{ key_mode }}"
     owner: "{{ key_owner }}"
     group: "{{ key_group }}"
-  when: cephx
+  when: 
+    - cephx
+    - instances is not defined
+
+- name: create rados gateway keyring for rgw instances
+  command: ceph --cluster {{ cluster }} --name client.bootstrap-rgw --keyring /var/lib/ceph/bootstrap-rgw/{{ cluster }}.keyring auth get-or-create client.rgw.{{ item | default(ansible_hostname) }} osd 'allow rwx' mon 'allow rw' -o /var/lib/ceph/radosgw/{{ cluster }}-rgw.{{ item | default(ansible_hostname) }}/keyring
+  args:
+    creates: /var/lib/ceph/radosgw/{{ cluster }}-rgw.{{ item | default(ansible_hostname) }}/keyring
+  changed_when: false
+  with_items: "{{ instances }}"
+  when:
+    - cephx
+    - instances is defined
+
 
 - name: ensure ceph-radosgw systemd unit file is present
   command: chkconfig --add ceph-radosgw
@@ -77,7 +116,23 @@
   changed_when: false
   when:
     - ansible_distribution != "Ubuntu"
-    - not use_systemd
+    - instances is not defined
+
+- name: activate rados gateway with sysvinit for rgw instances
+  file:
+    path: /var/lib/ceph/radosgw/{{ cluster }}-rgw.{{ item[0] | default(ansible_hostname) }}/{{ item[1] }}
+    state: touch
+    owner: "{{ activate_file_owner }}"
+    group: "{{ activate_file_group }}"
+    mode: "{{ activate_file_mode }}"
+  with_nested:
+    - "{{ instances }}"
+    - [ done, sysvinit]
+  changed_when: false
+  when:
+    - ansible_distribution != "Ubuntu"
+    - instances is defined
+
 
 - name: generate rados gateway sudoers file
   template:
